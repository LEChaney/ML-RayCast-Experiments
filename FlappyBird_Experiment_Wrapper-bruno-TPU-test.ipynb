{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlappyBird Experiment Wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEChaney/ML-RayCast-Experiments/blob/master/FlappyBird_Experiment_Wrapper-bruno-TPU-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzTlE0wWB2mn",
        "colab_type": "code",
        "outputId": "78251cef-b0e4-4a8d-c211-e2a0513b8511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Dependencies\n",
        "%tensorflow_version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2SzQ4_BJ0i",
        "colab_type": "code",
        "outputId": "ec078b93-675c-4a37-c6f3-a27a7dc96097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pygame\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uYauIb0H3KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilkVOEQoNeH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#import pygame\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "#pygame.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3t-fvH8M760",
        "colab_type": "code",
        "outputId": "ca2195ab-82cb-4301-8396-4a5b3d17050b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/LEChaney/ML-RayCast-Experiments\n",
        "%cd ML-RayCast-Experiments/DeepLearningFlappyBird/\n",
        "# !python deep_q_network.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML-RayCast-Experiments'...\n",
            "remote: Enumerating objects: 158, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/158)\u001b[K\rremote: Counting objects:   1% (2/158)\u001b[K\rremote: Counting objects:   2% (4/158)\u001b[K\rremote: Counting objects:   3% (5/158)\u001b[K\rremote: Counting objects:   4% (7/158)\u001b[K\rremote: Counting objects:   5% (8/158)\u001b[K\rremote: Counting objects:   6% (10/158)\u001b[K\rremote: Counting objects:   7% (12/158)\u001b[K\rremote: Counting objects:   8% (13/158)\u001b[K\rremote: Counting objects:   9% (15/158)\u001b[K\rremote: Counting objects:  10% (16/158)\u001b[K\rremote: Counting objects:  11% (18/158)\u001b[K\rremote: Counting objects:  12% (19/158)\u001b[K\rremote: Counting objects:  13% (21/158)\u001b[K\rremote: Counting objects:  14% (23/158)\u001b[K\rremote: Counting objects:  15% (24/158)\u001b[K\rremote: Counting objects:  16% (26/158)\u001b[K\rremote: Counting objects:  17% (27/158)\u001b[K\rremote: Counting objects:  18% (29/158)\u001b[K\rremote: Counting objects:  19% (31/158)\u001b[K\rremote: Counting objects:  20% (32/158)\u001b[K\rremote: Counting objects:  21% (34/158)\u001b[K\rremote: Counting objects:  22% (35/158)\u001b[K\rremote: Counting objects:  23% (37/158)\u001b[K\rremote: Counting objects:  24% (38/158)\u001b[K\rremote: Counting objects:  25% (40/158)\u001b[K\rremote: Counting objects:  26% (42/158)\u001b[K\rremote: Counting objects:  27% (43/158)\u001b[K\rremote: Counting objects:  28% (45/158)\u001b[K\rremote: Counting objects:  29% (46/158)\u001b[K\rremote: Counting objects:  30% (48/158)\u001b[K\rremote: Counting objects:  31% (49/158)\u001b[K\rremote: Counting objects:  32% (51/158)\u001b[K\rremote: Counting objects:  33% (53/158)\u001b[K\rremote: Counting objects:  34% (54/158)\u001b[K\rremote: Counting objects:  35% (56/158)\u001b[K\rremote: Counting objects:  36% (57/158)\u001b[K\rremote: Counting objects:  37% (59/158)\u001b[K\rremote: Counting objects:  38% (61/158)\u001b[K\rremote: Counting objects:  39% (62/158)\u001b[K\rremote: Counting objects:  40% (64/158)\u001b[K\rremote: Counting objects:  41% (65/158)\u001b[K\rremote: Counting objects:  42% (67/158)\u001b[K\rremote: Counting objects:  43% (68/158)\u001b[K\rremote: Counting objects:  44% (70/158)\u001b[K\rremote: Counting objects:  45% (72/158)\u001b[K\rremote: Counting objects:  46% (73/158)\u001b[K\rremote: Counting objects:  47% (75/158)\u001b[K\rremote: Counting objects:  48% (76/158)\u001b[K\rremote: Counting objects:  49% (78/158)\u001b[K\rremote: Counting objects:  50% (79/158)\u001b[K\rremote: Counting objects:  51% (81/158)\u001b[K\rremote: Counting objects:  52% (83/158)\u001b[K\rremote: Counting objects:  53% (84/158)\u001b[K\rremote: Counting objects:  54% (86/158)\u001b[K\rremote: Counting objects:  55% (87/158)\u001b[K\rremote: Counting objects:  56% (89/158)\u001b[K\rremote: Counting objects:  57% (91/158)\u001b[K\rremote: Counting objects:  58% (92/158)\u001b[K\rremote: Counting objects:  59% (94/158)\u001b[K\rremote: Counting objects:  60% (95/158)\u001b[K\rremote: Counting objects:  61% (97/158)\u001b[K\rremote: Counting objects:  62% (98/158)\u001b[K\rremote: Counting objects:  63% (100/158)\u001b[K\rremote: Counting objects:  64% (102/158)\u001b[K\rremote: Counting objects:  65% (103/158)\u001b[K\rremote: Counting objects:  66% (105/158)\u001b[K\rremote: Counting objects:  67% (106/158)\u001b[K\rremote: Counting objects:  68% (108/158)\u001b[K\rremote: Counting objects:  69% (110/158)\u001b[K\rremote: Counting objects:  70% (111/158)\u001b[K\rremote: Counting objects:  71% (113/158)\u001b[K\rremote: Counting objects:  72% (114/158)\u001b[K\rremote: Counting objects:  73% (116/158)\u001b[K\rremote: Counting objects:  74% (117/158)\u001b[K\rremote: Counting objects:  75% (119/158)\u001b[K\rremote: Counting objects:  76% (121/158)\u001b[K\rremote: Counting objects:  77% (122/158)\u001b[K\rremote: Counting objects:  78% (124/158)\u001b[K\rremote: Counting objects:  79% (125/158)\u001b[K\rremote: Counting objects:  80% (127/158)\u001b[K\rremote: Counting objects:  81% (128/158)\u001b[K\rremote: Counting objects:  82% (130/158)\u001b[K\rremote: Counting objects:  83% (132/158)\u001b[K\rremote: Counting objects:  84% (133/158)\u001b[K\rremote: Counting objects:  85% (135/158)\u001b[K\rremote: Counting objects:  86% (136/158)\u001b[K\rremote: Counting objects:  87% (138/158)\u001b[K\rremote: Counting objects:  88% (140/158)\u001b[K\rremote: Counting objects:  89% (141/158)\u001b[K\rremote: Counting objects:  90% (143/158)\u001b[K\rremote: Counting objects:  91% (144/158)\u001b[K\rremote: Counting objects:  92% (146/158)\u001b[K\rremote: Counting objects:  93% (147/158)\u001b[K\rremote: Counting objects:  94% (149/158)\u001b[K\rremote: Counting objects:  95% (151/158)\u001b[K\rremote: Counting objects:  96% (152/158)\u001b[K\rremote: Counting objects:  97% (154/158)\u001b[K\rremote: Counting objects:  98% (155/158)\u001b[K\rremote: Counting objects:  99% (157/158)\u001b[K\rremote: Counting objects: 100% (158/158)\u001b[K\rremote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 158 (delta 86), reused 103 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (158/158), 6.24 MiB | 25.15 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/ML-RayCast-Experiments/DeepLearningFlappyBird/ML-RayCast-Experiments/DeepLearningFlappyBird/ML-RayCast-Experiments/DeepLearningFlappyBird/ML-RayCast-Experiments/DeepLearningFlappyBird\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rapp_2Il7zh",
        "colab_type": "code",
        "outputId": "eef2b6ba-bca5-4347-c82e-0e8d519e4a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append(\"game/\")\n",
        "import wrapped_flappy_bird as game\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import os\n",
        "import contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def dummy_context_mgr():\n",
        "    yield None\n",
        "\n",
        "GAME = 'bird' # the name of the game being played for log files\n",
        "ACTIONS = 2 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVE = 10000. # timesteps to observe before training\n",
        "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.01, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, stride):\n",
        "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
        "\n",
        "def using_TPU():\n",
        "    if 'COLAB_TPU_ADDR' in os.environ:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "if using_TPU():\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\n",
        "    tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "    print('using TPU')\n",
        "      \n",
        "with strategy.scope() if using_TPU() else dummy_context_mgr():\n",
        "    def createNetwork():\n",
        "        # network weights\n",
        "        W_conv1 = weight_variable([8, 8, 4, 32])\n",
        "        b_conv1 = bias_variable([32])\n",
        "\n",
        "        W_conv2 = weight_variable([4, 4, 32, 64])\n",
        "        b_conv2 = bias_variable([64])\n",
        "\n",
        "        W_conv3 = weight_variable([3, 3, 64, 64])\n",
        "        b_conv3 = bias_variable([64])\n",
        "\n",
        "        W_fc1 = weight_variable([1600, 512])\n",
        "        b_fc1 = bias_variable([512])\n",
        "\n",
        "        W_fc2 = weight_variable([512, ACTIONS])\n",
        "        b_fc2 = bias_variable([ACTIONS])\n",
        "\n",
        "        # input layer\n",
        "        s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
        "\n",
        "        # hidden layers\n",
        "        h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
        "        h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
        "        #h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "        h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
        "        #h_pool3 = max_pool_2x2(h_conv3)\n",
        "\n",
        "        #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
        "        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
        "\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # readout layer\n",
        "        readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "\n",
        "        return s, readout, h_fc1\n",
        "\n",
        "    def trainNetwork(s, readout, h_fc1, sess):\n",
        "        # define the cost function\n",
        "        a = tf.placeholder(\"float\", [None, ACTIONS])\n",
        "        y = tf.placeholder(\"float\", [None])\n",
        "        readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
        "        cost = tf.reduce_mean(tf.square(y - readout_action))\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cost)\n",
        "        train_step = optimizer._distributed_apply(strategy, grads_and_vars)\n",
        "\n",
        "        # open up a game state to communicate with emulator\n",
        "        game_state = game.GameState()\n",
        "\n",
        "        # store the previous observations in replay memory\n",
        "        D = deque()\n",
        "\n",
        "        # printing\n",
        "        a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
        "        h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
        "\n",
        "        # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        do_nothing = np.zeros(ACTIONS)\n",
        "        do_nothing[0] = 1\n",
        "        x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
        "        x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "        ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
        "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
        "\n",
        "        # saving and loading networks\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
        "        # if checkpoint and checkpoint.model_checkpoint_path:\n",
        "        #     saver.restore(sess, checkpoint.model_checkpoint_path)\n",
        "        #     print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
        "        # else:\n",
        "        #     print(\"Could not find old network weights\")\n",
        "\n",
        "        # start training\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        t = 0\n",
        "        while \"flappy bird\" != \"angry bird\":\n",
        "            # choose an action epsilon greedily\n",
        "            readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
        "            a_t = np.zeros([ACTIONS])\n",
        "            action_index = 0\n",
        "            if t % FRAME_PER_ACTION == 0:\n",
        "                if random.random() <= epsilon:\n",
        "                    # print(\"----------Random Action----------\")\n",
        "                    action_index = random.randrange(ACTIONS)\n",
        "                    a_t[random.randrange(ACTIONS)] = 1\n",
        "                else:\n",
        "                    action_index = np.argmax(readout_t)\n",
        "                    a_t[action_index] = 1\n",
        "            else:\n",
        "                a_t[0] = 1 # do nothing\n",
        "\n",
        "            # scale down epsilon\n",
        "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "            # run the selected action and observe next state and reward\n",
        "            x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
        "            x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "            ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
        "            x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
        "            #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
        "            s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
        "\n",
        "            # store the transition in D\n",
        "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
        "            if len(D) > REPLAY_MEMORY:\n",
        "                D.popleft()\n",
        "\n",
        "            # only train if done observing\n",
        "            if t > OBSERVE:\n",
        "                # sample a minibatch to train on\n",
        "                minibatch = random.sample(D, BATCH)\n",
        "\n",
        "                # get the batch variables\n",
        "                s_j_batch = [d[0] for d in minibatch]\n",
        "                a_batch = [d[1] for d in minibatch]\n",
        "                r_batch = [d[2] for d in minibatch]\n",
        "                s_j1_batch = [d[3] for d in minibatch]\n",
        "\n",
        "                y_batch = []\n",
        "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
        "                for i in range(0, len(minibatch)):\n",
        "                    terminal = minibatch[i][4]\n",
        "                    # if terminal, only equals reward\n",
        "                    if terminal:\n",
        "                        y_batch.append(r_batch[i])\n",
        "                    else:\n",
        "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
        "\n",
        "                # perform gradient step\n",
        "                train_step.run(feed_dict = {\n",
        "                    y : y_batch,\n",
        "                    a : a_batch,\n",
        "                    s : s_j_batch}\n",
        "                )\n",
        "\n",
        "            # update the old values\n",
        "            s_t = s_t1\n",
        "            t += 1\n",
        "\n",
        "            # save progress every 10000 iterations\n",
        "            if t % 10000 == 0:\n",
        "                saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
        "\n",
        "            # print info\n",
        "            state = \"\"\n",
        "            if t <= OBSERVE:\n",
        "                state = \"observe\"\n",
        "            elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "                state = \"explore\"\n",
        "            else:\n",
        "                state = \"train\"\n",
        "\n",
        "            if t % 10000 == 0:\n",
        "                print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "                    \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "                    \"/ Q_MAX %e\" % np.max(readout_t))\n",
        "            # write info to files\n",
        "            '''\n",
        "            if t % 10000 <= 100:\n",
        "                a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
        "                h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
        "                cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
        "        \n",
        "            '''\n",
        "\n",
        "    def playGame():\n",
        "        sess = tf.InteractiveSession()\n",
        "        s, readout, h_fc1 = createNetwork()\n",
        "        trainNetwork(s, readout, h_fc1, sess)\n",
        "\n",
        "    def main():\n",
        "        playGame()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "INFO:tensorflow:Initializing the TPU system.\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.30.191.146:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10896786932289626259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5960944615810306130)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1743508852913162670)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 856948461577933407)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2502679422369885928)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9868928214655520186)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8222782048640353349)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 12192823062453915131)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5390721457202509395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 11011308427858797959)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 884901798275262430)\n",
            "using TPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fe140cc34f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-fe140cc34f3a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-fe140cc34f3a>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trainNetwork() missing 1 required positional argument: 'strategy'"
          ]
        }
      ]
    }
  ]
}