{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlappyBird Experiment Wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEChaney/ML-RayCast-Experiments/blob/master/FlappyBird_Experiment_Wrapper-bruno-TPU-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzTlE0wWB2mn",
        "colab_type": "code",
        "outputId": "78251cef-b0e4-4a8d-c211-e2a0513b8511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Dependencies\n",
        "%tensorflow_version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2SzQ4_BJ0i",
        "colab_type": "code",
        "outputId": "7163b908-f48b-4a5f-f920-9138221db7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pygame\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uYauIb0H3KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilkVOEQoNeH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#import pygame\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "#pygame.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3t-fvH8M760",
        "colab_type": "code",
        "outputId": "110201a7-923f-481f-f318-2fbe74d45613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/LEChaney/ML-RayCast-Experiments\n",
        "%cd ML-RayCast-Experiments/DeepLearningFlappyBird/\n",
        "# !python deep_q_network.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML-RayCast-Experiments'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/152)\u001b[K\rremote: Counting objects:   1% (2/152)\u001b[K\rremote: Counting objects:   2% (4/152)\u001b[K\rremote: Counting objects:   3% (5/152)\u001b[K\rremote: Counting objects:   4% (7/152)\u001b[K\rremote: Counting objects:   5% (8/152)\u001b[K\rremote: Counting objects:   6% (10/152)\u001b[K\rremote: Counting objects:   7% (11/152)\u001b[K\rremote: Counting objects:   8% (13/152)\u001b[K\rremote: Counting objects:   9% (14/152)\u001b[K\rremote: Counting objects:  10% (16/152)\u001b[K\rremote: Counting objects:  11% (17/152)\u001b[K\rremote: Counting objects:  12% (19/152)\u001b[K\rremote: Counting objects:  13% (20/152)\u001b[K\rremote: Counting objects:  14% (22/152)\u001b[K\rremote: Counting objects:  15% (23/152)\u001b[K\rremote: Counting objects:  16% (25/152)\u001b[K\rremote: Counting objects:  17% (26/152)\u001b[K\rremote: Counting objects:  18% (28/152)\u001b[K\rremote: Counting objects:  19% (29/152)\u001b[K\rremote: Counting objects:  20% (31/152)\u001b[K\rremote: Counting objects:  21% (32/152)\u001b[K\rremote: Counting objects:  22% (34/152)\u001b[K\rremote: Counting objects:  23% (35/152)\u001b[K\rremote: Counting objects:  24% (37/152)\u001b[K\rremote: Counting objects:  25% (38/152)\u001b[K\rremote: Counting objects:  26% (40/152)\u001b[K\rremote: Counting objects:  27% (42/152)\u001b[K\rremote: Counting objects:  28% (43/152)\u001b[K\rremote: Counting objects:  29% (45/152)\u001b[K\rremote: Counting objects:  30% (46/152)\u001b[K\rremote: Counting objects:  31% (48/152)\u001b[K\rremote: Counting objects:  32% (49/152)\u001b[K\rremote: Counting objects:  33% (51/152)\u001b[K\rremote: Counting objects:  34% (52/152)\u001b[K\rremote: Counting objects:  35% (54/152)\u001b[K\rremote: Counting objects:  36% (55/152)\u001b[K\rremote: Counting objects:  37% (57/152)\u001b[K\rremote: Counting objects:  38% (58/152)\u001b[K\rremote: Counting objects:  39% (60/152)\u001b[K\rremote: Counting objects:  40% (61/152)\u001b[K\rremote: Counting objects:  41% (63/152)\u001b[K\rremote: Counting objects:  42% (64/152)\u001b[K\rremote: Counting objects:  43% (66/152)\u001b[K\rremote: Counting objects:  44% (67/152)\u001b[K\rremote: Counting objects:  45% (69/152)\u001b[K\rremote: Counting objects:  46% (70/152)\u001b[K\rremote: Counting objects:  47% (72/152)\u001b[K\rremote: Counting objects:  48% (73/152)\u001b[K\rremote: Counting objects:  49% (75/152)\u001b[K\rremote: Counting objects:  50% (76/152)\u001b[K\rremote: Counting objects:  51% (78/152)\u001b[K\rremote: Counting objects:  52% (80/152)\u001b[K\rremote: Counting objects:  53% (81/152)\u001b[K\rremote: Counting objects:  54% (83/152)\u001b[K\rremote: Counting objects:  55% (84/152)\u001b[K\rremote: Counting objects:  56% (86/152)\u001b[K\rremote: Counting objects:  57% (87/152)\u001b[K\rremote: Counting objects:  58% (89/152)\u001b[K\rremote: Counting objects:  59% (90/152)\u001b[K\rremote: Counting objects:  60% (92/152)\u001b[K\rremote: Counting objects:  61% (93/152)\u001b[K\rremote: Counting objects:  62% (95/152)\u001b[K\rremote: Counting objects:  63% (96/152)\u001b[K\rremote: Counting objects:  64% (98/152)\u001b[K\rremote: Counting objects:  65% (99/152)\u001b[K\rremote: Counting objects:  66% (101/152)\u001b[K\rremote: Counting objects:  67% (102/152)\u001b[K\rremote: Counting objects:  68% (104/152)\u001b[K\rremote: Counting objects:  69% (105/152)\u001b[K\rremote: Counting objects:  70% (107/152)\u001b[K\rremote: Counting objects:  71% (108/152)\u001b[K\rremote: Counting objects:  72% (110/152)\u001b[K\rremote: Counting objects:  73% (111/152)\u001b[K\rremote: Counting objects:  74% (113/152)\u001b[K\rremote: Counting objects:  75% (114/152)\u001b[K\rremote: Counting objects:  76% (116/152)\u001b[K\rremote: Counting objects:  77% (118/152)\u001b[K\rremote: Counting objects:  78% (119/152)\u001b[K\rremote: Counting objects:  79% (121/152)\u001b[K\rremote: Counting objects:  80% (122/152)\u001b[K\rremote: Counting objects:  81% (124/152)\u001b[K\rremote: Counting objects:  82% (125/152)\u001b[K\rremote: Counting objects:  83% (127/152)\u001b[K\rremote: Counting objects:  84% (128/152)\u001b[K\rremote: Counting objects:  85% (130/152)\u001b[K\rremote: Counting objects:  86% (131/152)\u001b[K\rremote: Counting objects:  87% (133/152)\u001b[K\rremote: Counting objects:  88% (134/152)\u001b[K\rremote: Counting objects:  89% (136/152)\u001b[K\rremote: Counting objects:  90% (137/152)\u001b[K\rremote: Counting objects:  91% (139/152)\u001b[K\rremote: Counting objects:  92% (140/152)\u001b[K\rremote: Counting objects:  93% (142/152)\u001b[K\rremote: Counting objects:  94% (143/152)\u001b[K\rremote: Counting objects:  95% (145/152)\u001b[K\rremote: Counting objects:  96% (146/152)\u001b[K\rremote: Counting objects:  97% (148/152)\u001b[K\rremote: Counting objects:  98% (149/152)\u001b[K\rremote: Counting objects:  99% (151/152)\u001b[K\rremote: Counting objects: 100% (152/152)\u001b[K\rremote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 152 (delta 81), reused 103 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (152/152), 6.24 MiB | 24.12 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "/content/ML-RayCast-Experiments/DeepLearningFlappyBird/ML-RayCast-Experiments/DeepLearningFlappyBird/ML-RayCast-Experiments/DeepLearningFlappyBird\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rapp_2Il7zh",
        "colab_type": "code",
        "outputId": "b590a03d-64cb-4536-f33c-d5794a47ec58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append(\"game/\")\n",
        "import wrapped_flappy_bird as game\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import os\n",
        "import contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def dummy_context_mgr():\n",
        "    yield None\n",
        "\n",
        "GAME = 'bird' # the name of the game being played for log files\n",
        "ACTIONS = 2 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVE = 10000. # timesteps to observe before training\n",
        "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.01, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, stride):\n",
        "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
        "\n",
        "def using_TPU():\n",
        "    if 'COLAB_TPU_ADDR' in os.environ:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "if using_TPU():\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\n",
        "    tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "    print('using TPU')\n",
        "      \n",
        "with strategy.scope() if using_TPU() else dummy_context_mgr():\n",
        "    def createNetwork():\n",
        "        # network weights\n",
        "        W_conv1 = weight_variable([8, 8, 4, 32])\n",
        "        b_conv1 = bias_variable([32])\n",
        "\n",
        "        W_conv2 = weight_variable([4, 4, 32, 64])\n",
        "        b_conv2 = bias_variable([64])\n",
        "\n",
        "        W_conv3 = weight_variable([3, 3, 64, 64])\n",
        "        b_conv3 = bias_variable([64])\n",
        "\n",
        "        W_fc1 = weight_variable([1600, 512])\n",
        "        b_fc1 = bias_variable([512])\n",
        "\n",
        "        W_fc2 = weight_variable([512, ACTIONS])\n",
        "        b_fc2 = bias_variable([ACTIONS])\n",
        "\n",
        "        # input layer\n",
        "        s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
        "\n",
        "        # hidden layers\n",
        "        h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
        "        h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
        "        #h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "        h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
        "        #h_pool3 = max_pool_2x2(h_conv3)\n",
        "\n",
        "        #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
        "        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
        "\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # readout layer\n",
        "        readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "\n",
        "        return s, readout, h_fc1\n",
        "\n",
        "    def trainNetwork(s, readout, h_fc1, sess, strategy):\n",
        "        # define the cost function\n",
        "        a = tf.placeholder(\"float\", [None, ACTIONS])\n",
        "        y = tf.placeholder(\"float\", [None])\n",
        "        readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
        "        cost = tf.reduce_mean(tf.square(y - readout_action))\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cost)\n",
        "        train_step = optimizer._distributed_apply(strategy, grads_and_vars)\n",
        "\n",
        "        # open up a game state to communicate with emulator\n",
        "        game_state = game.GameState()\n",
        "\n",
        "        # store the previous observations in replay memory\n",
        "        D = deque()\n",
        "\n",
        "        # printing\n",
        "        a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
        "        h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
        "\n",
        "        # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        do_nothing = np.zeros(ACTIONS)\n",
        "        do_nothing[0] = 1\n",
        "        x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
        "        x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "        ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
        "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
        "\n",
        "        # saving and loading networks\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
        "        # if checkpoint and checkpoint.model_checkpoint_path:\n",
        "        #     saver.restore(sess, checkpoint.model_checkpoint_path)\n",
        "        #     print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
        "        # else:\n",
        "        #     print(\"Could not find old network weights\")\n",
        "\n",
        "        # start training\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        t = 0\n",
        "        while \"flappy bird\" != \"angry bird\":\n",
        "            # choose an action epsilon greedily\n",
        "            readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
        "            a_t = np.zeros([ACTIONS])\n",
        "            action_index = 0\n",
        "            if t % FRAME_PER_ACTION == 0:\n",
        "                if random.random() <= epsilon:\n",
        "                    # print(\"----------Random Action----------\")\n",
        "                    action_index = random.randrange(ACTIONS)\n",
        "                    a_t[random.randrange(ACTIONS)] = 1\n",
        "                else:\n",
        "                    action_index = np.argmax(readout_t)\n",
        "                    a_t[action_index] = 1\n",
        "            else:\n",
        "                a_t[0] = 1 # do nothing\n",
        "\n",
        "            # scale down epsilon\n",
        "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "            # run the selected action and observe next state and reward\n",
        "            x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
        "            x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "            ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
        "            x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
        "            #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
        "            s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
        "\n",
        "            # store the transition in D\n",
        "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
        "            if len(D) > REPLAY_MEMORY:\n",
        "                D.popleft()\n",
        "\n",
        "            # only train if done observing\n",
        "            if t > OBSERVE:\n",
        "                # sample a minibatch to train on\n",
        "                minibatch = random.sample(D, BATCH)\n",
        "\n",
        "                # get the batch variables\n",
        "                s_j_batch = [d[0] for d in minibatch]\n",
        "                a_batch = [d[1] for d in minibatch]\n",
        "                r_batch = [d[2] for d in minibatch]\n",
        "                s_j1_batch = [d[3] for d in minibatch]\n",
        "\n",
        "                y_batch = []\n",
        "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
        "                for i in range(0, len(minibatch)):\n",
        "                    terminal = minibatch[i][4]\n",
        "                    # if terminal, only equals reward\n",
        "                    if terminal:\n",
        "                        y_batch.append(r_batch[i])\n",
        "                    else:\n",
        "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
        "\n",
        "                # perform gradient step\n",
        "                train_step.run(feed_dict = {\n",
        "                    y : y_batch,\n",
        "                    a : a_batch,\n",
        "                    s : s_j_batch}\n",
        "                )\n",
        "\n",
        "            # update the old values\n",
        "            s_t = s_t1\n",
        "            t += 1\n",
        "\n",
        "            # save progress every 10000 iterations\n",
        "            if t % 10000 == 0:\n",
        "                saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
        "\n",
        "            # print info\n",
        "            state = \"\"\n",
        "            if t <= OBSERVE:\n",
        "                state = \"observe\"\n",
        "            elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "                state = \"explore\"\n",
        "            else:\n",
        "                state = \"train\"\n",
        "\n",
        "            if t % 10000 == 0:\n",
        "                print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "                    \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "                    \"/ Q_MAX %e\" % np.max(readout_t))\n",
        "            # write info to files\n",
        "            '''\n",
        "            if t % 10000 <= 100:\n",
        "                a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
        "                h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
        "                cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
        "        \n",
        "            '''\n",
        "\n",
        "    def playGame():\n",
        "        sess = tf.InteractiveSession()\n",
        "        s, readout, h_fc1 = createNetwork()\n",
        "        trainNetwork(s, readout, h_fc1, sess)\n",
        "\n",
        "    def main():\n",
        "        playGame()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "INFO:tensorflow:Initializing the TPU system.\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.30.191.146:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10896786932289626259)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5960944615810306130)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1743508852913162670)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 856948461577933407)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2502679422369885928)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9868928214655520186)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8222782048640353349)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 12192823062453915131)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5390721457202509395)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 11011308427858797959)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 884901798275262430)\n",
            "using TPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fe140cc34f3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-fe140cc34f3a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mplayGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fe140cc34f3a>\u001b[0m in \u001b[0;36mplayGame\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mtrainNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_fc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: trainNetwork() missing 1 required positional argument: 'strategy'"
          ]
        }
      ]
    }
  ]
}