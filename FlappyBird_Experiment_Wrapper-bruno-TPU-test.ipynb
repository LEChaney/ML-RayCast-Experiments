{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlappyBird Experiment Wrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LEChaney/ML-RayCast-Experiments/blob/master/FlappyBird_Experiment_Wrapper-bruno-TPU-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzTlE0wWB2mn",
        "colab_type": "code",
        "outputId": "78251cef-b0e4-4a8d-c211-e2a0513b8511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Dependencies\n",
        "%tensorflow_version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently selected TF version: 1.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb2SzQ4_BJ0i",
        "colab_type": "code",
        "outputId": "48936888-2da6-4fa0-a996-148de41c3189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install pygame\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uYauIb0H3KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilkVOEQoNeH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#import pygame\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "#pygame.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3t-fvH8M760",
        "colab_type": "code",
        "outputId": "7d036a1d-f325-4451-af14-3b62c95c9f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/LEChaney/ML-RayCast-Experiments\n",
        "%cd ML-RayCast-Experiments/DeepLearningFlappyBird/\n",
        "# !python deep_q_network.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ML-RayCast-Experiments'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 146 (delta 77), reused 103 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 6.24 MiB | 26.06 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "/content/ML-RayCast-Experiments/DeepLearningFlappyBird\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rapp_2Il7zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "6f1b25b4-dbeb-4067-ae85-02f7f70964f8"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append(\"game/\")\n",
        "import wrapped_flappy_bird as game\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import os\n",
        "import contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def dummy_context_mgr():\n",
        "    yield None\n",
        "\n",
        "GAME = 'bird' # the name of the game being played for log files\n",
        "ACTIONS = 2 # number of valid actions\n",
        "GAMMA = 0.99 # decay rate of past observations\n",
        "OBSERVE = 10000. # timesteps to observe before training\n",
        "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 32 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.01, shape = shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, stride):\n",
        "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
        "\n",
        "def using_TPU():\n",
        "    if 'COLAB_TPU_ADDR' in os.environ:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "if using_TPU():\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\n",
        "    tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "    print('using TPU')\n",
        "      \n",
        "with strategy.scope() if using_TPU() else dummy_context_mgr():\n",
        "    def createNetwork():\n",
        "        # network weights\n",
        "        W_conv1 = weight_variable([8, 8, 4, 32])\n",
        "        b_conv1 = bias_variable([32])\n",
        "\n",
        "        W_conv2 = weight_variable([4, 4, 32, 64])\n",
        "        b_conv2 = bias_variable([64])\n",
        "\n",
        "        W_conv3 = weight_variable([3, 3, 64, 64])\n",
        "        b_conv3 = bias_variable([64])\n",
        "\n",
        "        W_fc1 = weight_variable([1600, 512])\n",
        "        b_fc1 = bias_variable([512])\n",
        "\n",
        "        W_fc2 = weight_variable([512, ACTIONS])\n",
        "        b_fc2 = bias_variable([ACTIONS])\n",
        "\n",
        "        # input layer\n",
        "        s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
        "\n",
        "        # hidden layers\n",
        "        h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n",
        "        h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
        "        #h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "        h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
        "        #h_pool3 = max_pool_2x2(h_conv3)\n",
        "\n",
        "        #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
        "        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
        "\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # readout layer\n",
        "        readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "\n",
        "        return s, readout, h_fc1\n",
        "\n",
        "    def trainNetwork(s, readout, h_fc1, sess, strategy):\n",
        "        # define the cost function\n",
        "        a = tf.placeholder(\"float\", [None, ACTIONS])\n",
        "        y = tf.placeholder(\"float\", [None])\n",
        "        readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
        "        cost = tf.reduce_mean(tf.square(y - readout_action))\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cost)\n",
        "        train_step = optimizer._distributed_apply(strategy, grads_and_vars)\n",
        "\n",
        "        # open up a game state to communicate with emulator\n",
        "        game_state = game.GameState()\n",
        "\n",
        "        # store the previous observations in replay memory\n",
        "        D = deque()\n",
        "\n",
        "        # printing\n",
        "        a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
        "        h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
        "\n",
        "        # get the first state by doing nothing and preprocess the image to 80x80x4\n",
        "        do_nothing = np.zeros(ACTIONS)\n",
        "        do_nothing[0] = 1\n",
        "        x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
        "        x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "        ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
        "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
        "\n",
        "        # saving and loading networks\n",
        "        saver = tf.train.Saver()\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "        checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
        "        # if checkpoint and checkpoint.model_checkpoint_path:\n",
        "        #     saver.restore(sess, checkpoint.model_checkpoint_path)\n",
        "        #     print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
        "        # else:\n",
        "        #     print(\"Could not find old network weights\")\n",
        "\n",
        "        # start training\n",
        "        epsilon = INITIAL_EPSILON\n",
        "        t = 0\n",
        "        while \"flappy bird\" != \"angry bird\":\n",
        "            # choose an action epsilon greedily\n",
        "            readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
        "            a_t = np.zeros([ACTIONS])\n",
        "            action_index = 0\n",
        "            if t % FRAME_PER_ACTION == 0:\n",
        "                if random.random() <= epsilon:\n",
        "                    # print(\"----------Random Action----------\")\n",
        "                    action_index = random.randrange(ACTIONS)\n",
        "                    a_t[random.randrange(ACTIONS)] = 1\n",
        "                else:\n",
        "                    action_index = np.argmax(readout_t)\n",
        "                    a_t[action_index] = 1\n",
        "            else:\n",
        "                a_t[0] = 1 # do nothing\n",
        "\n",
        "            # scale down epsilon\n",
        "            if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "                epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "            # run the selected action and observe next state and reward\n",
        "            x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
        "            x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
        "            ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
        "            x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
        "            #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n",
        "            s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
        "\n",
        "            # store the transition in D\n",
        "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
        "            if len(D) > REPLAY_MEMORY:\n",
        "                D.popleft()\n",
        "\n",
        "            # only train if done observing\n",
        "            if t > OBSERVE:\n",
        "                # sample a minibatch to train on\n",
        "                minibatch = random.sample(D, BATCH)\n",
        "\n",
        "                # get the batch variables\n",
        "                s_j_batch = [d[0] for d in minibatch]\n",
        "                a_batch = [d[1] for d in minibatch]\n",
        "                r_batch = [d[2] for d in minibatch]\n",
        "                s_j1_batch = [d[3] for d in minibatch]\n",
        "\n",
        "                y_batch = []\n",
        "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
        "                for i in range(0, len(minibatch)):\n",
        "                    terminal = minibatch[i][4]\n",
        "                    # if terminal, only equals reward\n",
        "                    if terminal:\n",
        "                        y_batch.append(r_batch[i])\n",
        "                    else:\n",
        "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
        "\n",
        "                # perform gradient step\n",
        "                train_step.run(feed_dict = {\n",
        "                    y : y_batch,\n",
        "                    a : a_batch,\n",
        "                    s : s_j_batch}\n",
        "                )\n",
        "\n",
        "            # update the old values\n",
        "            s_t = s_t1\n",
        "            t += 1\n",
        "\n",
        "            # save progress every 10000 iterations\n",
        "            if t % 10000 == 0:\n",
        "                saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
        "\n",
        "            # print info\n",
        "            state = \"\"\n",
        "            if t <= OBSERVE:\n",
        "                state = \"observe\"\n",
        "            elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "                state = \"explore\"\n",
        "            else:\n",
        "                state = \"train\"\n",
        "\n",
        "            if t % 10000 == 0:\n",
        "                print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n",
        "                    \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n",
        "                    \"/ Q_MAX %e\" % np.max(readout_t))\n",
        "            # write info to files\n",
        "            '''\n",
        "            if t % 10000 <= 100:\n",
        "                a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
        "                h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
        "                cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
        "        \n",
        "            '''\n",
        "\n",
        "    def playGame():\n",
        "        sess = tf.InteractiveSession()\n",
        "        s, readout, h_fc1 = createNetwork()\n",
        "        trainNetwork(s, readout, h_fc1, sess, )\n",
        "\n",
        "    def main():\n",
        "        playGame()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 1.9.6\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-99376b5f8673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0musing_TPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mTPU_WORKER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPU_WORKER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'using_TPU' is not defined"
          ]
        }
      ]
    }
  ]
}