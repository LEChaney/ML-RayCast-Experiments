{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function I stole from somewhere to draw a line.\n",
    "# I also repurposed it for the ray casting.\n",
    "def get_line(start, end):\n",
    "    \"\"\"Bresenham's Line Algorithm\n",
    "    Produces a list of tuples from start and end\n",
    " \n",
    "    >>> points1 = get_line((0, 0), (3, 4))\n",
    "    >>> points2 = get_line((3, 4), (0, 0))\n",
    "    >>> assert(set(points1) == set(points2))\n",
    "    >>> print points1\n",
    "    [(0, 0), (1, 1), (1, 2), (2, 3), (3, 4)]\n",
    "    >>> print points2\n",
    "    [(3, 4), (2, 3), (1, 2), (1, 1), (0, 0)]\n",
    "    \"\"\"  \n",
    "    # Setup initial conditions\n",
    "    x1, y1 = start\n",
    "    x2, y2 = end\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    " \n",
    "    # Determine how steep the line is\n",
    "    is_steep = abs(dy) > abs(dx)\n",
    " \n",
    "    # Rotate line\n",
    "    if is_steep:\n",
    "        x1, y1 = y1, x1\n",
    "        x2, y2 = y2, x2\n",
    " \n",
    "    # Swap start and end points if necessary and store swap state\n",
    "    swapped = False\n",
    "    if x1 > x2:\n",
    "        x1, x2 = x2, x1\n",
    "        y1, y2 = y2, y1\n",
    "        swapped = True\n",
    " \n",
    "    # Recalculate differentials\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    " \n",
    "    # Calculate error\n",
    "    error = int(dx / 2.0)\n",
    "    ystep = 1 if y1 < y2 else -1\n",
    " \n",
    "    # Iterate over bounding box generating points between start and end\n",
    "    y = y1\n",
    "    points = []\n",
    "    for x in range(x1, x2 + 1):\n",
    "        coord = (y, x) if is_steep else (x, y)\n",
    "        points.append(coord)\n",
    "        error -= abs(dy)\n",
    "        if error < 0:\n",
    "            y += ystep\n",
    "            error += dx\n",
    " \n",
    "    # Reverse the list if the coordinates were swapped\n",
    "    if swapped:\n",
    "        points.reverse()\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x287D220DAC8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads the current test map.\n",
    "# This map should probably be randomly generated, otherwise we're biasing our training a lot to this specific map layout.\n",
    "img = load_img('line trace map.png')\n",
    "img = img.resize((128, 128))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very lazy training set generation code.\n",
    "# Uses a lot of memory as it just loads all the images into a big array in memory.\n",
    "# Could be saved to file instead, then streamed in, there's probably an easy way to stream in files with Keras.\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "hit_locations = []\n",
    "headings = []\n",
    "line_length = math.sqrt(img.width ** 2 + img.height ** 2)\n",
    "compass_radius = 10\n",
    "for _ in range(100000): # Change this to reduce the number of training examples / memory load\n",
    "    placed_compass = False\n",
    "    while not placed_compass:\n",
    "        # Reset image\n",
    "        new_image = img.copy()\n",
    "        out_hit = (0, 0)\n",
    "        \n",
    "        # Get random line, long enough to span entire image\n",
    "        start = (random.randint(0, img.width - 1), random.randint(0, img.height - 1))\n",
    "        heading = 2 * math.pi * random.random()\n",
    "        end = (int(line_length * math.cos(heading) + start[0]), int(line_length * math.sin(heading) + start[1]))\n",
    "        \n",
    "        # Try paint compass and perform line trace from end\n",
    "        out_hit = start\n",
    "        line_coords = get_line(start, end)\n",
    "        for i, pixel_coord in enumerate(line_coords):\n",
    "            \n",
    "            # Retry if outside of image\n",
    "            x, y = pixel_coord\n",
    "            if x < 0 or x >= new_image.width or y < 0 or y >= new_image.height:\n",
    "                break\n",
    "                \n",
    "            # Keep track of current end of line for hit results\n",
    "            out_hit = pixel_coord\n",
    "                \n",
    "            # Retry if placed inside solid\n",
    "            if new_image.getpixel(pixel_coord) != (255, 255, 255):\n",
    "                break\n",
    "                \n",
    "            # Success if we painted all pixels without hitting a solid\n",
    "            vpixel = np.array(pixel_coord)\n",
    "            vstart = np.array(start)\n",
    "            if np.linalg.norm(vpixel - vstart) > 2 * compass_radius:\n",
    "                placed_compass = True\n",
    "                continue\n",
    "                \n",
    "            # Select pixel color for north and south poles\n",
    "            if np.linalg.norm(vpixel - vstart) < compass_radius:\n",
    "                pixel_color = (127, 127, 127)\n",
    "            else:\n",
    "                pixel_color = (255, 0, 0)\n",
    "            \n",
    "            # Place new pixel on line\n",
    "            new_image.putpixel(pixel_coord, pixel_color)\n",
    "    \n",
    "    hit_locations.append(out_hit)\n",
    "    images.append(new_image)\n",
    "    headings.append(heading)\n",
    "\n",
    "# Paint hit locations\n",
    "# for i, hit_loc in enumerate(hit_locations):\n",
    "#     images[i].putpixel(hit_loc, (0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x287D22AECC0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample with compass drawn on map (compass is currently only 1 pixel wide, may be unfair representation of a game object)\n",
    "images[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split (Keras also makes its own train / validation split on the training data)\n",
    "TRAIN_PERCENT = 0.80\n",
    "\n",
    "split_loc = int(len(images) * TRAIN_PERCENT)\n",
    "x_train = np.array([np.array(image) for image in images[:split_loc]])\n",
    "y_train = np.array([np.array(hit_loc) for hit_loc in hit_locations[:split_loc]])\n",
    "x_test = np.array([np.array(image) for image in images[split_loc:]])\n",
    "y_test = np.array([np.array(hit_loc) for hit_loc in hit_locations[split_loc:]])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "y_train[:, 0] /= (x_train.shape[2] - 1)\n",
    "y_train[:, 1] /= (x_train.shape[1] - 1)\n",
    "y_test[:, 0] /= (x_test.shape[2] - 1)\n",
    "y_test[:, 1] /= (x_test.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PERCENT = 0.80\n",
    "\n",
    "# split_loc = int(len(images) * TRAIN_PERCENT)\n",
    "# x_train = np.array([np.array(image) for image in images[:split_loc]])\n",
    "# y_train = np.array([np.array(heading) for heading in headings[:split_loc]])\n",
    "# x_test = np.array([np.array(image) for image in images[split_loc:]])\n",
    "# y_test = np.array([np.array(heading) for heading in headings[split_loc:]])\n",
    "\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train = x_train / 255.0\n",
    "# x_test = x_test / 255.0\n",
    "\n",
    "# y_train = y_train.astype('float32')\n",
    "# y_test = y_test.astype('float32')\n",
    "# y_train /= 2 * math.pi\n",
    "# y_test /= 2 * math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 128, 128, 3)\n",
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2892365b710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# larger view of a sample from the training set\n",
    "print(x_train.shape)\n",
    "print(x_train[0, 100, 100])\n",
    "plt.imshow(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 2)\n",
      "[0.56692916 0.10236221]\n"
     ]
    }
   ],
   "source": [
    "# Sample of training labels\n",
    "print(y_train.shape)\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 128, 128, 3)\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Checking test sets shape\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from coord import CoordinateChannel2D\n",
    "\n",
    "# Function that builds the neural network model\n",
    "def get_model(input_shape):\n",
    "    ip = keras.layers.Input(shape=input_shape)\n",
    "    x = keras.layers.Conv2D(32, (3, 3), strides=2, padding='same')(ip)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    x = keras.layers.Conv2D(32, (3, 3), strides=2, padding='same')(x)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = keras.layers.Dense(32)(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    x = keras.layers.Dense(2)(x)\n",
    "#     x = keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = keras.models.Model(ip, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow113\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                65568     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 75,778\n",
      "Trainable params: 75,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Builds the model and sets up the optimizer for our problem.\n",
    "# Prints summary of model.\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = get_model(x_train[0].shape)\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow113\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 53599 samples, validate on 26401 samples\n",
      "Epoch 1/100\n",
      "53599/53599 [==============================] - 24s 457us/step - loss: 0.0586 - val_loss: 0.0329\n",
      "Epoch 2/100\n",
      "53599/53599 [==============================] - 19s 363us/step - loss: 0.0383 - val_loss: 0.0156\n",
      "Epoch 3/100\n",
      "53599/53599 [==============================] - 20s 371us/step - loss: 0.0114 - val_loss: 0.0096\n",
      "Epoch 4/100\n",
      "53599/53599 [==============================] - 20s 368us/step - loss: 0.0084 - val_loss: 0.0072\n",
      "Epoch 5/100\n",
      "53599/53599 [==============================] - 19s 363us/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 6/100\n",
      "53599/53599 [==============================] - 19s 362us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 7/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0041 - val_loss: 0.0045\n",
      "Epoch 8/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 9/100\n",
      "53599/53599 [==============================] - 20s 369us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 10/100\n",
      "53599/53599 [==============================] - 20s 369us/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 11/100\n",
      "53599/53599 [==============================] - 20s 366us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 12/100\n",
      "53599/53599 [==============================] - 19s 363us/step - loss: 0.0027 - val_loss: 0.0031\n",
      "Epoch 13/100\n",
      "53599/53599 [==============================] - 19s 360us/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 14/100\n",
      "53599/53599 [==============================] - 19s 362us/step - loss: 0.0025 - val_loss: 0.0029\n",
      "Epoch 15/100\n",
      "53599/53599 [==============================] - 19s 360us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 16/100\n",
      "53599/53599 [==============================] - 19s 362us/step - loss: 0.0023 - val_loss: 0.0027\n",
      "Epoch 17/100\n",
      "53599/53599 [==============================] - 19s 361us/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 18/100\n",
      "53599/53599 [==============================] - 19s 360us/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 19/100\n",
      "53599/53599 [==============================] - 19s 361us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 20/100\n",
      "53599/53599 [==============================] - 19s 364us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 21/100\n",
      "53599/53599 [==============================] - 20s 366us/step - loss: 0.0019 - val_loss: 0.0025\n",
      "Epoch 22/100\n",
      "53599/53599 [==============================] - 20s 378us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 23/100\n",
      "53599/53599 [==============================] - 20s 375us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 24/100\n",
      "53599/53599 [==============================] - 20s 366us/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 25/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 26/100\n",
      "53599/53599 [==============================] - 19s 362us/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 27/100\n",
      "53599/53599 [==============================] - 19s 362us/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 28/100\n",
      "53599/53599 [==============================] - 19s 361us/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 29/100\n",
      "53599/53599 [==============================] - 20s 364us/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 30/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 31/100\n",
      "53599/53599 [==============================] - 20s 367us/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 32/100\n",
      "53599/53599 [==============================] - 20s 367us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 33/100\n",
      "53599/53599 [==============================] - 20s 366us/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 34/100\n",
      "53599/53599 [==============================] - 19s 361us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 35/100\n",
      "53599/53599 [==============================] - 19s 360us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 36/100\n",
      "53599/53599 [==============================] - 19s 361us/step - loss: 0.0014 - val_loss: 0.0019\n",
      "Epoch 37/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0014 - val_loss: 0.0020\n",
      "Epoch 38/100\n",
      "53599/53599 [==============================] - 20s 365us/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 39/100\n",
      "53599/53599 [==============================] - 20s 367us/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 40/100\n",
      "53408/53599 [============================>.] - ETA: 0s - loss: 0.0013"
     ]
    }
   ],
   "source": [
    "# Runs the training for 100 epochs (complete runs over training set) or until early stopping criteria is met\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "history = model.fit(x_train, y_train, validation_split=0.33, epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# Summarize performance\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run final evaluation on our hold out test set\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_test)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# rand_im_idx = random.randint(0, x_test.shape[0] - 1)\n",
    "# test_img = x_test[rand_im_idx]\n",
    "# print(\"True Value: {}\".format(y_test[rand_im_idx] * 360))\n",
    "# print(\"Prediction: {}\".format(y_hat[rand_im_idx] * 360))\n",
    "# print(\"Error: {}\".format((y_hat[rand_im_idx] - y_test[rand_im_idx]) * 360))\n",
    "# print(\"Avg Error: {}\".format(np.mean(np.abs(y_test - y_hat.flatten())) * 360))\n",
    "# plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays random result from the test set\n",
    "\n",
    "import random\n",
    "\n",
    "rand_im_idx = random.randint(0, x_test.shape[0] - 1)\n",
    "test_img = x_test[rand_im_idx]\n",
    "print(y_hat[rand_im_idx])\n",
    "hit_x = y_hat[rand_im_idx][0]\n",
    "hit_x *= test_img.shape[1] - 1 # [0.0..1.0] x Image width\n",
    "hit_x = int(round(hit_x))\n",
    "hit_y = y_hat[rand_im_idx][1]\n",
    "hit_y *= test_img.shape[0] - 1 # [0.0..1.0] x Image height\n",
    "hit_y = int(round(hit_y))\n",
    "print((hit_x, hit_y))\n",
    "t_hit_x = y_test[rand_im_idx][0]\n",
    "t_hit_x *= test_img.shape[1] - 1 # [0.0..1.0] x Image width\n",
    "t_hit_x = int(round(t_hit_x))\n",
    "t_hit_y = y_test[rand_im_idx][1]\n",
    "t_hit_y *= test_img.shape[0] - 1 # [0.0..1.0] x Image height\n",
    "t_hit_y = int(round(t_hit_y))\n",
    "print((t_hit_x, t_hit_y))\n",
    "test_img[hit_y, hit_x] = (0., 1., 0)\n",
    "test_img[t_hit_y, t_hit_x] = (1., 0., 0.)\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow113]",
   "language": "python",
   "name": "conda-env-tensorflow113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
